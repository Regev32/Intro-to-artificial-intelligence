{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGFRAgEfNqP6"
   },
   "source": [
    "# Linear and Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZBbdBvnNqP7"
   },
   "source": [
    "## Linear Regreesion Using ScikitLearn <a class=\"anchor\" id=\"scikit\"></a>\n",
    " <a class=\"anchor\" id=\"LinearRegression\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-KxWIE-bNqP7"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QS9MH1npNqP7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# inline plot\n",
    "%matplotlib inline\n",
    "# default figure size\n",
    "matplotlib.rcParams['figure.figsize'] = (20, 10)\n",
    "# to make our sets reproducible\n",
    "np.random.seed(42)\n",
    "# %config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DPYdwb1SNqP7"
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple, Union, Optional, Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPZvEy8UNqP7"
   },
   "source": [
    "### Data generation <a class=\"anchor\" id=\"linear_data_generation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UgqJsV5SNqP7"
   },
   "source": "$$Y = 3 \\cdot X + 4 + \\epsilon | \\epsilon \\sim  N(0,1)$$"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e_XP4HreNqP8"
   },
   "outputs": [],
   "source": [
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Trrf9KO8NqP8"
   },
   "source": [
    "## Linear Regreesion Using ScikitLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "akQUYSxbNqP8"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)  # Trying to solve the (X^TX)^-1X^Ty PROBLEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D9dOCYI9NqP8"
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "print(\"Intercept:\", lin_reg.intercept_[0])\n",
    "print(\"Slope:\", lin_reg.coef_[0][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ChNXsIB1NqP9"
   },
   "source": [
    "### Converging with different learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NAVNWByxNqP9"
   },
   "outputs": [],
   "source": [
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "X_new = 2 * np.random.rand(20, 1)\n",
    "\n",
    "X_b = np.c_[np.ones((100, 1)), X]\n",
    "X_new_b = np.c_[np.ones((20, 1)), X_new]\n",
    "\n",
    "def plot_gradient_descent(theta, eta:float, theta_path=None, n_iterations: int = 1000):\n",
    "    # number of samples\n",
    "    m = len(X_b)\n",
    "    # plot the samples with their corrseponding labels\n",
    "    plt.plot(X, y, \"b.\")\n",
    "\n",
    "    for iteration in range(n_iterations):\n",
    "\n",
    "        if iteration < 10:\n",
    "            # make predictions\n",
    "            y_predict = X_new_b.dot(theta)\n",
    "            style = \"b-\" if iteration > 0 else \"r--\"\n",
    "            # plot the predictions\n",
    "            plt.plot(X_new, y_predict, style)\n",
    "\n",
    "        # gradient is computed using all datapoints\n",
    "        gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "        theta = theta - eta * gradients\n",
    "        # add new\n",
    "        if theta_path is not None:\n",
    "            theta_path.append(theta)\n",
    "\n",
    "    plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "    plt.axis([0, 2, -5, 15])\n",
    "    plt.title(r\"$\\eta = {}$\".format(eta), fontsize=16)\n",
    "\n",
    "theta_path_bgd = []\n",
    "\n",
    "np.random.seed(42)\n",
    "theta = np.random.randn(2,1)  # random initialization\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.subplot(131); plot_gradient_descent(theta, eta=0.02)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.subplot(132); plot_gradient_descent(theta, eta=0.1, theta_path=theta_path_bgd)\n",
    "plt.subplot(133); plot_gradient_descent(theta, eta=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZpyWTax5NqP9"
   },
   "source": [
    "## Stochastic Gradient Descent using scikit  <a class=\"anchor\" id=\"sgd\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fR5vxa3ONqP9"
   },
   "outputs": [],
   "source": [
    "# SGD = Stochastic Gradient Descent\n",
    "# The model updates its parameters after each sample (or mini-batch),\n",
    "# using a learning rate schedule to gradually reduce update strength.\n",
    "\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "# Create and train the SGD regressor\n",
    "sgd_reg = SGDRegressor(\n",
    "    max_iter=1000,      # number of epochs (passes over data)\n",
    "    eta0=0.1,           # initial learning rate\n",
    "    learning_rate='constant',  # keep learning rate fixed\n",
    ")\n",
    "\n",
    "sgd_reg.fit(X, y.ravel())\n",
    "\n",
    "# Print learned parameters\n",
    "print(f\"Intercept: {sgd_reg.intercept_[0]:.6f}\")\n",
    "print(f\"Slope: {sgd_reg.coef_[0]:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uisk2ieJNqP9"
   },
   "source": [
    "## Polynomial Regression <a class=\"anchor\" id=\"polynomial_regression\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nTuegHLkNqP-"
   },
   "outputs": [],
   "source": [
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3  # -3 <= x <= 3\n",
    "y = 0.5 * X**2 - 0.5 * X + 2 + np.random.randn(m, 1) / 4\n",
    "\n",
    "# test data\n",
    "X_test = 6 * np.random.rand(30, 1) - 3\n",
    "y_test = 0.5 * X_test**2 - 0.5 * X_test + 2 + np.random.randn(30, 1) / 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o2HMiCamNqP-"
   },
   "source": [
    "Generate polynomial features (degree 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i0O1i4eKNqP-"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)  # [x] -> [x, x**2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2VoaOaoNNqP-"
   },
   "outputs": [],
   "source": [
    "print(f\"\"\" data:\n",
    "{X[:5]}\n",
    "\n",
    "data with deg 2 polynomial features:\n",
    "\n",
    "{X_poly[:5]}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hJaCR3dENqP-"
   },
   "outputs": [],
   "source": [
    "## fitting  linear regression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_poly, y)\n",
    "\n",
    "print(f'''\n",
    "features coefficients: {lin_reg.coef_}\n",
    "intercept: {lin_reg.intercept_}\n",
    "''')\n",
    "\n",
    "## Evaluation\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# train predictions\n",
    "train_predictions = lin_reg.predict(X_poly)\n",
    "train_mse = mean_squared_error(y, train_predictions)\n",
    "\n",
    "# evaluation prediction\n",
    "predictions = lin_reg.predict(poly_features.transform(X_test))\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "\n",
    "rmse, train_rmse = np.sqrt(mse), np.sqrt(train_mse)\n",
    "print(f\"\"\"\n",
    "    train RMSE: {train_rmse}\n",
    "    test RMSE: {rmse}\n",
    "\"\"\")\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "train_r2 = r2_score(y_true=y,y_pred=train_predictions)\n",
    "r2 = r2_score(y_true=y_test,y_pred=predictions)\n",
    "print(f\"\"\"\n",
    "    train R2: {train_r2}\n",
    "    test R2: {r2}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bShZv3NgNqP-"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "# plot train and test\n",
    "plt.plot(X_test, y_test, \"go\", label='test')\n",
    "plt.plot(X, y, \"b.\", label='train')\n",
    "\n",
    "# plot the polynomial\n",
    "xs = np.linspace(-3, 3, 20)[:, np.newaxis]\n",
    "ys = lin_reg.predict(poly_features.transform(xs))\n",
    "plt.plot(xs, ys, \"r-\", label='fit')\n",
    "\n",
    "plt.axis([-2, 3, 1, 6])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nxhYmu0cNqP_"
   },
   "source": [
    "## Learning Curves <a class=\"anchor\" id=\"learning_curves\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4YQHQzT6NqP_"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# split test as the largest X values (sorted), keep train as the rest\n",
    "def split_sorted(X: np.ndarray, y: np.ndarray, test_size: float):\n",
    "    idxs = np.argsort(X.reshape(-1))\n",
    "    n_test = int(len(y) * test_size)\n",
    "    if n_test == 0:\n",
    "        raise ValueError(\"test_size too small for dataset size\")\n",
    "    train_idxs = idxs[:-n_test]\n",
    "    test_idxs = np.setdiff1d(np.arange(len(y)), train_idxs)\n",
    "    return X[train_idxs], X[test_idxs], y[train_idxs], y[test_idxs]\n",
    "\n",
    "def plot_learning_curves(model, X, y, split_sort: bool = False):\n",
    "    # split to train and test\n",
    "    if split_sort:\n",
    "        X_train, X_val, y_train, y_val = split_sorted(X, y, test_size=0.2)\n",
    "    else:\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "    train_errors, val_errors = [], []\n",
    "\n",
    "    # increase the train sequentially\n",
    "    for m in range(1, len(X_train) + 1):\n",
    "        model.fit(X_train[:m], y_train[:m])\n",
    "        y_train_predict = model.predict(X_train[:m])\n",
    "        y_val_predict = model.predict(X_val)\n",
    "        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n",
    "        val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
    "\n",
    "    # one figure with two subplots\n",
    "    fig, axes = plt.subplots(ncols=2, figsize=(8, 3))\n",
    "\n",
    "    # left: data + current fit\n",
    "    axes[0].plot(X_train, y_train, \"r.\", label=\"train\")\n",
    "    axes[0].plot(X_val, y_val, \"bo\", label=\"validation\")\n",
    "    xs = np.linspace(X.min(), X.max(), 200).reshape(-1, 1)\n",
    "    ys = model.predict(xs).ravel()\n",
    "    axes[0].plot(xs, ys, \"g-\", label=\"fit\")\n",
    "    axes[0].legend(loc=\"upper right\", fontsize=10)\n",
    "\n",
    "    # right: learning curves (RMSE)\n",
    "    axes[1].plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\n",
    "    axes[1].plot(np.sqrt(val_errors), \"b-\", linewidth=2, label=\"val\")\n",
    "    axes[1].legend(loc=\"upper right\", fontsize=10)\n",
    "    axes[1].set_xlabel(\"Training set size\")\n",
    "    axes[1].set_ylabel(\"RMSE\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# -------------------------\n",
    "# 1) Data\n",
    "# -------------------------\n",
    "np.random.seed(42)\n",
    "m = 50\n",
    "X = np.linspace(-3, 3, m).reshape(-1, 1)  # -3 <= x <= 3\n",
    "y = 0.5 * X**2 + X + 2 + 0.3 * np.random.randn(m, 1)\n",
    "\n",
    "# grid for smooth curves\n",
    "xs = np.linspace(X.min(), X.max(), 400).reshape(-1, 1)\n",
    "\n",
    "# -------------------------\n",
    "# 2) Fit models and compute BIC\n",
    "# -------------------------\n",
    "def fit_poly_and_bic(X, y, degree):\n",
    "    \"\"\"Fit polynomial regression of given degree; return model and BIC.\"\"\"\n",
    "    model = make_pipeline(\n",
    "        PolynomialFeatures(degree=degree, include_bias=True),  # include_bias=True -> intercept handled in features\n",
    "        LinearRegression(fit_intercept=False)                  # so we avoid double-intercept\n",
    "    )\n",
    "    model.fit(X, y)\n",
    "    y_hat = model.predict(X)\n",
    "\n",
    "    # Residual sum of squares\n",
    "    rss = float(np.sum((y - y_hat)**2))\n",
    "    n = len(y)\n",
    "\n",
    "    # Number of parameters = number of polynomial terms up to 'degree'\n",
    "    # With include_bias=True in PolynomialFeatures, k = degree + 1 (for 1D X)\n",
    "    k = degree + 1\n",
    "\n",
    "    # Gaussian-error BIC: n*ln(RSS/n) + k*ln(n)\n",
    "    bic = n * np.log(rss / n) + k * np.log(n)\n",
    "    return model, bic, rss\n",
    "\n",
    "max_degree = 15\n",
    "models = {}\n",
    "bics = []\n",
    "rss_list = []\n",
    "\n",
    "for deg in range(1, max_degree + 1):\n",
    "    model, bic, rss = fit_poly_and_bic(X, y, deg)\n",
    "    models[deg] = model\n",
    "    bics.append(bic)\n",
    "    rss_list.append(rss)\n",
    "\n",
    "degrees = np.arange(1, max_degree + 1)\n",
    "bics = np.array(bics)\n",
    "\n",
    "best_deg = int(degrees[np.argmin(bics)])\n",
    "\n",
    "# Choose canonical examples\n",
    "under_deg = 1\n",
    "over_deg = max_degree\n",
    "good_deg = best_deg\n",
    "\n",
    "print(f\"Best degree by BIC: {best_deg}\")\n",
    "\n",
    "# -------------------------\n",
    "# 3) Plot underfit vs good fit vs overfit\n",
    "# -------------------------\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 3), sharey=True)\n",
    "cases = [(\"Underfit\", under_deg), (\"Good (BIC-best)\", good_deg), (\"Overfit\", over_deg)]\n",
    "\n",
    "for ax, (title, deg) in zip(axes, cases):\n",
    "    ax.scatter(X, y, s=18, alpha=0.7, label=\"data\")\n",
    "    ys = models[deg].predict(xs).ravel()\n",
    "    ax.plot(xs, ys, lw=2, label=f\"degree={deg}\")\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.grid(alpha=0.2)\n",
    "axes[0].set_ylabel(\"y\")\n",
    "axes[1].legend(loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "L5_kh23nd0O1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# -------------------------\n",
    "# 4) BIC “elbow” plot\n",
    "# -------------------------\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(degrees, bics, marker=\"o\")\n",
    "plt.xlabel(\"Polynomial degree\")\n",
    "plt.ylabel(\"BIC (lower is better)\")\n",
    "plt.title(\"Model selection by BIC\")\n",
    "plt.grid(alpha=0.3)\n",
    "# mark best\n",
    "plt.scatter([best_deg], [bics[best_deg - 1]], s=80, zorder=3)\n",
    "plt.annotate(f\"best = {best_deg}\", xy=(best_deg, bics[best_deg - 1]),\n",
    "             xytext=(best_deg + 0.5, bics[best_deg - 1] + 0.5),\n",
    "             arrowprops=dict(arrowstyle=\"->\", lw=1))\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "id": "0maXIk4ba08C"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HAo4jfH8NqQA"
   },
   "source": [
    "## Regularized regression <a class=\"anchor\" id=\"regularization\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJqpq4EVNqQA"
   },
   "source": [
    "### Ridge <a class=\"anchor\" id=\"ridge_lasso_elastic\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N7fmB_pFNqQA"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge_reg = Ridge(alpha=1)\n",
    "ridge_reg.fit(X, y)\n",
    "ridge_reg.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9NlwzkCKNqQA"
   },
   "source": [
    "### Using different regularization parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UryioGClNqQA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from typing import Callable, List, Dict\n",
    "\n",
    "# Generate data\n",
    "np.random.seed(42)\n",
    "m = 50\n",
    "X = np.linspace(-3, 3, m).reshape(-1, 1)\n",
    "y = 0.5 * X**2 + X + 2 + 0.5 * np.random.randn(m, 1)\n",
    "\n",
    "# Utility function\n",
    "def plot_model(model_class: Callable, polynomial: bool, alphas: List[float], title: str, **model_kargs: Dict):\n",
    "    \"\"\"\n",
    "    Show the effect of regularization (alpha) for linear or polynomial models.\n",
    "    \"\"\"\n",
    "    X_grid = np.linspace(X.min(), X.max(), 300).reshape(-1, 1)\n",
    "    styles = (\"b-\", \"g--\", \"r:\")\n",
    "    lw_for = lambda a: 2 if a > 0 else 1\n",
    "\n",
    "    for alpha, style in zip(alphas, styles):\n",
    "        base = model_class(alpha=alpha, **model_kargs)\n",
    "        if polynomial:\n",
    "            model = Pipeline([\n",
    "                (\"poly\", PolynomialFeatures(degree=10, include_bias=False)),\n",
    "                (\"scaler\", StandardScaler()),\n",
    "                (\"ridge\", base)\n",
    "            ])\n",
    "        else:\n",
    "            model = base\n",
    "\n",
    "        model.fit(X, y)\n",
    "        y_pred = model.predict(X_grid)\n",
    "        plt.plot(X_grid, y_pred, style, linewidth=lw_for(alpha),\n",
    "                 label=fr\"$\\alpha = {alpha}$\")\n",
    "\n",
    "    plt.scatter(X, y, color=\"black\", s=20, alpha=0.6, label=\"data\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"$x$\")\n",
    "    plt.ylabel(\"$y$\")\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(alpha=0.25)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dHZlc_9rNqQA"
   },
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plot_model(Ridge, polynomial=False, alphas=(0, 10, 100),\n",
    "            title=\"Linear (Underfit)\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_model(Ridge, polynomial=True, alphas=(0, 1e-3, 1),\n",
    "            title=\"Polynomial (Regularization Effect)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EP2b1oeQNqQA"
   },
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O5ZD5uCENqQA"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso_reg = Lasso(alpha=0.1)\n",
    "lasso_reg.fit(X, y)\n",
    "lasso_reg.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fwt9koF_NqQA"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "plt.subplot(121)\n",
    "plot_model(Lasso,polynomial=False,alphas=(1e-7, 0.1, 1),title=\"Linear (Lasso)\",random_state=42)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "\n",
    "plt.subplot(122)\n",
    "plot_model(Lasso,polynomial=True,alphas=(1e-7, 1e-3, 1),title=\"Polynomial (Lasso)\",random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3h4VzJHPNqQB"
   },
   "source": [
    "## Elatic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SMWoYIH0NqQB"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# ElasticNet do alpha(rho||theta||_1 + (1-rho)||theta||_2^2 /2) so l1_ratio = rho\n",
    "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)\n",
    "elastic_net.fit(X, y)\n",
    "elastic_net.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "plt.subplot(121)\n",
    "plot_model(ElasticNet,polynomial=False,alphas=(1e-7, 0.1, 1),title=\"Linear (ElasticNet)\",random_state=42, l1_ratio=0.5)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "\n",
    "plt.subplot(122)\n",
    "plot_model(ElasticNet,polynomial=True,alphas=(1e-7, 1e-3, 1),title=\"Polynomial (ElasticNet)\",random_state=42, l1_ratio=0.5)\n"
   ],
   "metadata": {
    "id": "IzUDe-lthJ8R"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TC0pAMcbNqQB"
   },
   "source": [
    "## Early Stopping <a class=\"anchor\" id=\"early_stopping\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GlO-_fJcNqQB"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = 2 + X + 0.5 * X**2 + np.random.randn(m, 1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X[:50], y[:50].ravel(), test_size=0.5, random_state=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r2qLEIQaNqQB"
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "# feature engineering\n",
    "poly_scaler = Pipeline([  # very important to set include_bias=False, because SGRegressor and LinearRegression add the intercept\n",
    "        (\"poly_features\", PolynomialFeatures(degree=90, include_bias=False)),\n",
    "        (\"std_scaler\", StandardScaler())\n",
    "    ])\n",
    "\n",
    "# train and transform on training set\n",
    "X_train_poly_scaled = poly_scaler.fit_transform(X_train)\n",
    "# only transform on test\n",
    "X_val_poly_scaled = poly_scaler.transform(X_val)\n",
    "\n",
    "# configure for one iteration only (one epoch) - so we can go over epochs manually\n",
    "sgd_reg = SGDRegressor(max_iter=1, tol=None, warm_start=True,  # max_iter: take one epoch, tol: no tolerance to stop algorithm\n",
    "                       penalty=None, learning_rate=\"constant\", eta0=0.0005, random_state=42)\n",
    "\n",
    "# warm_start=True means that each fit will use the previous learned fit\n",
    "\n",
    "minimum_val_error = float(\"inf\")\n",
    "best_epoch = None\n",
    "best_model = None\n",
    "for epoch in range(1000):\n",
    "    sgd_reg.fit(X_train_poly_scaled, y_train)\n",
    "    y_val_predict = sgd_reg.predict(X_val_poly_scaled)  # sgd.step()\n",
    "    val_error = mean_squared_error(y_val, y_val_predict)\n",
    "    if val_error < minimum_val_error:\n",
    "        minimum_val_error = val_error\n",
    "        best_epoch = epoch\n",
    "        best_model = deepcopy(sgd_reg)\n",
    "print('best model')\n",
    "print(best_model.coef_, best_model.intercept_)\n",
    "print()\n",
    "print(f\"best epoch: {best_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "collapsed": true,
    "id": "JN30xsrXNqQB"
   },
   "outputs": [],
   "source": [
    "sgd_reg = SGDRegressor(max_iter=1, tol=None, warm_start=True,\n",
    "                       penalty=None, learning_rate=\"constant\", eta0=0.0005, random_state=42)\n",
    "\n",
    "n_epochs = 500\n",
    "train_errors, val_errors = [], []\n",
    "for epoch in range(n_epochs):\n",
    "    sgd_reg.fit(X_train_poly_scaled, y_train)\n",
    "    y_train_predict = sgd_reg.predict(X_train_poly_scaled)\n",
    "    y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n",
    "    train_errors.append(mean_squared_error(y_train, y_train_predict))\n",
    "    val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
    "\n",
    "best_epoch = np.argmin(val_errors)\n",
    "best_val_rmse = np.sqrt(val_errors[best_epoch])\n",
    "\n",
    "plt.annotate('Best model',\n",
    "             xy=(best_epoch, best_val_rmse),\n",
    "             xytext=(best_epoch, best_val_rmse + 1),\n",
    "             ha=\"center\",\n",
    "             arrowprops=dict(facecolor='black', shrink=0.05),\n",
    "             fontsize=16,\n",
    "            )\n",
    "\n",
    "best_val_rmse -= 0.03  # just to make the graph look better\n",
    "plt.plot([0, n_epochs], [best_val_rmse, best_val_rmse], \"k:\", linewidth=2)  # the dotted line\n",
    "plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"Validation set\")\n",
    "plt.plot(np.sqrt(train_errors), \"r--\", linewidth=2, label=\"Training set\")\n",
    "plt.legend(loc=\"upper right\", fontsize=14)\n",
    "plt.xlabel(\"Epoch\", fontsize=14)\n",
    "plt.ylabel(\"RMSE\", fontsize=14)\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "oMuC7An8fwjn"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
